{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Earthquake detection using social media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'ETHZ-03-03-01 - Earthquake detection using social media'),\n",
    "                ('abstract', 'This application takes Twitter to generate a heatmap for earthquake detection'),\n",
    "                ('id', 'ewf-ethz-03-03-01')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDate = dict([('id', 'toDate'),\n",
    "               ('value', '2020-05-27T00:00:00Z'),\n",
    "               ('title', 'Final Date'),\n",
    "               ('abstract', 'Final date of the temporal interval of interest with the format [YYYY-MM-DDThh:mm:ssZ]')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dDays = dict([('id', 'dDays'),\n",
    "               ('value', '1'),\n",
    "               ('title', 'Delta days'),\n",
    "               ('abstract', 'Number of days of interest - start of temporal interval of interest = toDate - dDays')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltaDays = dict([('id', 'ltaDays'),\n",
    "               ('value', '15'),\n",
    "               ('title', 'LTA days'),\n",
    "               ('abstract', 'Number of days before final date used for LTA computation.')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input reference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_reference = 'dummy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append('/application/notebook/libexec/')\n",
    "\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import osgeo.ogr as ogr\n",
    "import osgeo.osr as osr\n",
    "\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CERTH_API = 'http://160.40.49.181:4000/tweet_provider_api?productType=japanEarthquakeTweets'\n",
    "\n",
    "BIN_INTERVAL = 5 #seconds\n",
    "BIN_UNIT = 60 #tweets per minute\n",
    "STA_INTERVAL = 60 #seconds\n",
    "LTA_INTERVAL = 60 * 60 #seconds\n",
    "m = 4\n",
    "b = 10\n",
    "\n",
    "AREA_WKT = \"POLYGON((141.8583984375 45.81673343092707,145.83544921875 43.88133803549814,143.99926757812497 39.93772807580545,140.73010253906247 33.79767248255881,129.111328125 30.038050457654403,129.60717773437497 37.279120989381234,141.8583984375 45.81673343092707))\"\n",
    "\n",
    "etc_path = \"/application/notebook/etc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dates\n",
    "\n",
    "ltadays = datetime.timedelta(days=int(ltaDays['value']))\n",
    "ddays = datetime.timedelta(days=int(dDays['value']))\n",
    "\n",
    "tf = datetime.datetime.strptime(toDate['value'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "ti = tf - ltadays \n",
    "\n",
    "tid = tf - ddays\n",
    "\n",
    "fromDate = {'value': ti.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}\n",
    "\n",
    "startDOI = {'value': tid.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}\n",
    "\n",
    "print(fromDate['value'], toDate['value'], startDOI['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"{}&fromDate={}&toDate={}\".format(CERTH_API, fromDate['value'], toDate['value'])\n",
    "r = requests.get(query)\n",
    "response = r.json()\n",
    "if response['total_results'] == len(response['results']):\n",
    "    results = response['results']\n",
    "\n",
    "FROM_DATE = pd.to_datetime(fromDate['value'])\n",
    "TO_DATE = pd.to_datetime(toDate['value'])\n",
    "\n",
    "START_DOI = pd.to_datetime(startDOI['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute earthquake detection index\n",
    "#### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bins(lower_bound, upper_bound, width):\n",
    "    \"\"\" create_bins returns an equal-width (distance) partitioning. \n",
    "        It returns an ascending list of tuples, representing the intervals.\n",
    "        A tuple bins[i], i.e. (bins[i][0], bins[i][1])  with i > 0 \n",
    "        and i < quantity, satisfies the following conditions:\n",
    "            (1) bins[i][0] + width == bins[i][1]\n",
    "            (2) bins[i-1][0] + width == bins[i][0] and\n",
    "                bins[i-1][1] + width == bins[i][1]\n",
    "    \"\"\"\n",
    "    \n",
    "    freq = \"{}s\".format(width)\n",
    "    bins = []\n",
    "    times = pd.date_range(start=lower_bound, end=upper_bound, freq=freq )\n",
    "    for i, t in enumerate(times):\n",
    "        if i + 1 != len(times):\n",
    "            bins.append((t, times[i+1]))\n",
    "    return bins\n",
    "\n",
    "def find_bin(value, bins):\n",
    "    \"\"\" bins is a list of tuples, like [(0,20), (20, 40), (40, 60)],\n",
    "        binning returns the smallest index i of bins so that\n",
    "        bin[i][0] <= value < bin[i][1]\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(bins)):\n",
    "        if bins[i][0] <= value < bins[i][1]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def get_coeficient_array(tweet_df, bins):\n",
    "    tweet_per_minute = [0] * len(bins)\n",
    "    counter = Counter(tweet_df['bin_index'])\n",
    "    for x, y in counter.items():\n",
    "        tweet_per_minute[x] = y * int(BIN_UNIT / BIN_INTERVAL)\n",
    "\n",
    "    product = {'time_window' : bins, 'tweets_per_minute':tweet_per_minute}\n",
    "    df = pd.DataFrame(product)\n",
    "\n",
    "    sta_window = int(STA_INTERVAL/BIN_INTERVAL) +1\n",
    "    lta_window = int(LTA_INTERVAL/BIN_INTERVAL) +1\n",
    "    df['STA'] = df.iloc[:,1].rolling(window=sta_window).mean()\n",
    "    df['LTA'] = df.iloc[:,1].rolling(window=lta_window).mean()\n",
    "\n",
    "    df['C'] = df['STA'] / (m * df['LTA'] + b)\n",
    "    \n",
    "    return df['C']\n",
    "\n",
    "def get_wkt(coordinates):\n",
    "    lat, long = coordinates.split(\" \")\n",
    "    wkt = \"POINT ({} {})\".format(long, lat)\n",
    "    return wkt \n",
    "\n",
    "def date_to_str_iso(date):\n",
    "    return date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def date_to_str(date):\n",
    "    return date.strftime('%Y%m%d%H%M')\n",
    "\n",
    "def create_zip_file(files, output_path):\n",
    "    with ZipFile(output_path, 'w') as myzip:\n",
    "        for file in files:\n",
    "            myzip.write(file, arcname=os.path.basename(file))\n",
    "    return output_path\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet dataframe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = create_bins(lower_bound=FROM_DATE,\n",
    "                   upper_bound=TO_DATE,\n",
    "                   width=BIN_INTERVAL)\n",
    "\n",
    "\n",
    "binned_weights = []\n",
    "wkt = []\n",
    "tweet_time = []\n",
    "\n",
    "for result in results:\n",
    "    pd_datetime = pd.to_datetime(result['timestamp'])\n",
    "    bin_index = find_bin(pd_datetime, bins)\n",
    "    wkt.append(get_wkt(result['coordinates']))\n",
    "    tweet_time.append(pd_datetime)\n",
    "    binned_weights.append(bin_index)\n",
    "    \n",
    "event_number = [0] * len(binned_weights)\n",
    "tweet_product = {'bin_index': binned_weights, 'wkt': wkt, 'tweet_time':tweet_time, 'event_number': event_number}\n",
    "tweet_df = pd.DataFrame(tweet_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = get_coeficient_array(tweet_df, bins)\n",
    "\n",
    "start_time = []\n",
    "stop_time = []\n",
    "\n",
    "event_number = 0\n",
    "event = False\n",
    "for idx, value in enumerate(C):\n",
    "\n",
    "    if (value >= 1 and not event):\n",
    "        event = True\n",
    "        event_number = event_number + 1\n",
    "\n",
    "        condition = tweet_df['bin_index'] == idx\n",
    "        condition_df = tweet_df[condition]\n",
    "        start_time.append(date_to_str_iso(min(condition_df['tweet_time'])))\n",
    "\n",
    "    if event:\n",
    "        condition = tweet_df['bin_index'] == idx\n",
    "        condition_df = tweet_df[condition]\n",
    "        tweet_df.loc[condition, 'event_number'] = [event_number] * len(condition_df)\n",
    "\n",
    "    if value <= 0.25 and event:\n",
    "        event = False\n",
    "\n",
    "    if (len(start_time) > len(stop_time)) and not event:\n",
    "        condition = tweet_df['bin_index'] == idx\n",
    "        condition_df = tweet_df[condition]\n",
    "        if len(condition_df['tweet_time']) > 0:\n",
    "            stop_time.append(date_to_str_iso(max(condition_df['tweet_time'])))\n",
    "\n",
    "if len(start_time) == len(stop_time) + 1:\n",
    "    stop_time.append(date_to_str_iso(TO_DATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earthquake events dataframe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wkt_csv = []\n",
    "start_csv = []\n",
    "stop_csv = []\n",
    "\n",
    "wkt_shp = []\n",
    "\n",
    "poly_area = ogr.CreateGeometryFromWkt(AREA_WKT)\n",
    "\n",
    "for n_event in range(len(start_time)):\n",
    "    \n",
    "    if(datetime.datetime.strptime(start_time[n_event], '%Y-%m-%dT%H:%M:%SZ') > tid):\n",
    "    \n",
    "        event_condition = tweet_df['event_number'] == n_event + 1\n",
    "        event_df = tweet_df[event_condition]\n",
    "\n",
    "        wkt_list = []\n",
    "        for wkt in event_df['wkt']:\n",
    "            point_geom = ogr.CreateGeometryFromWkt(wkt)\n",
    "            intersection = poly_area.Intersects(point_geom)\n",
    "\n",
    "            if intersection:\n",
    "                if wkt not in wkt_list:\n",
    "                    wkt_list.append(wkt)\n",
    "                wkt_shp.append(wkt)\n",
    "\n",
    "        wkt_csv.extend(wkt_list)\n",
    "        start_csv.extend([start_time[n_event]] * len(wkt_list))\n",
    "        stop_csv.extend([stop_time[n_event]] * len(wkt_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results creation and publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if wkt_csv:\n",
    "    \n",
    "    from_date = date_to_str(START_DOI)\n",
    "    to_date = date_to_str(TO_DATE)\n",
    "    \n",
    "    from_date_iso = date_to_str_iso(START_DOI)\n",
    "    to_date_iso = date_to_str_iso(TO_DATE)\n",
    "    \n",
    "    \n",
    "    ## csv file ##\n",
    "    \n",
    "    csv_product = {'start_time': start_csv, 'stop_time': stop_csv, 'wkt':wkt_csv}\n",
    "    csv_df = pd.DataFrame(csv_product)\n",
    "\n",
    "    \n",
    "    filename_template = '{}_{}_earthquake_{}'\n",
    "    filename = filename_template.format(from_date, to_date, 'events')\n",
    "    csv_df.to_csv(filename + '.csv', index=False)\n",
    "    \n",
    "    write_properties_file(filename + '.csv', from_date_iso, to_date_iso, AREA_WKT)\n",
    "    \n",
    "    ## shapefile ##\n",
    "    \n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "\n",
    "    filename = filename_template.format(from_date, to_date, 'heatmap')\n",
    "    # create the data source\n",
    "    data_source = driver.CreateDataSource(filename + \".shp\")\n",
    "\n",
    "    # create the spatial reference, WGS84\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)\n",
    "\n",
    "    # create the layer\n",
    "    layer = data_source.CreateLayer(\"earthquakes\", srs, ogr.wkbPoint)\n",
    "    layer.CreateField(ogr.FieldDefn('weight', ogr.OFTInteger))\n",
    "\n",
    "    for wkt in wkt_shp:\n",
    "        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "        feature.SetField(\"weight\", 1)\n",
    "        point = ogr.CreateGeometryFromWkt(wkt)\n",
    "        feature.SetGeometry(point)\n",
    "        layer.CreateFeature(feature)\n",
    "        feature = None\n",
    "    # Save and close the data source\n",
    "    data_source = None\n",
    "    \n",
    "    \n",
    "    tmpqml_rel_path = 'tmp.qml'\n",
    "    \n",
    "    tmpqml_path = os.path.join(etc_path, tmpqml_rel_path)\n",
    "    \n",
    "    if not(os.path.isfile(tmpqml_path)): # when running locally\n",
    "        tmpqml_path = os.path.join(os.path.dirname(os.getcwd()), 'etc', 'tmp.qml')\n",
    "        \n",
    "    #print(os.path.isfile(tmpqml_path), tmpqml_path)\n",
    "    \n",
    "    \n",
    "    shutil.copy(tmpqml_path, filename + '.qml')\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in os.walk('.'):\n",
    "        break\n",
    "\n",
    "    zip_list = []\n",
    "    for file in filenames:\n",
    "        if filename in file and '.csv' not in file:\n",
    "            zip_list.append(os.path.join(dirpath, file))\n",
    "    create_zip_file(zip_list, filename + '.zip') \n",
    "\n",
    "    for file in zip_list:\n",
    "        os.remove(file)\n",
    "    \n",
    "    write_properties_file(filename + '.zip', from_date_iso, to_date_iso, AREA_WKT)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
